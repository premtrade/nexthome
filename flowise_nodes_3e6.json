[
  {
    "id": "conversationChain_0",
    "position": {
      "x": 863.8847744360903,
      "y": 116.24736842105261
    },
    "type": "customNode",
    "data": {
      "label": "Conversation Chain",
      "name": "conversationChain",
      "version": 3,
      "type": "ConversationChain",
      "icon": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chains/ConversationChain/conv.svg",
      "category": "Chains",
      "description": "Chat models specific conversational chain with memory",
      "baseClasses": [
        "ConversationChain",
        "LLMChain",
        "BaseChain",
        "Runnable"
      ],
      "inputs": {
        "model": "{{chatOpenAICustom_0.data.instance}}",
        "memory": "{{bufferMemory_0.data.instance}}",
        "chatPromptTemplate": "",
        "inputModeration": "",
        "systemMessagePrompt": "You are a real estate SEO expert. Your task is to generate a compelling, SEO-optimized property description, meta title, and meta description based on the provided property details.\n\nOutput MUST be a valid JSON object with the following keys:\n- seo_description: An engaging description (approx 200-300 words) using keywords naturally.\n- meta_title: A punchy meta title under 60 characters.\n- meta_description: A concise summary under 160 characters.\n\nInput details:\n```\n",
        "systemMessage": "You are a real estate SEO expert. Your task is to generate a compelling, SEO-optimized property description, meta title, and meta description based on the provided property details.\n\nOutput MUST be a valid JSON object with the following keys:\n- seo_description: An engaging description (approx 200-300 words) using keywords naturally.\n- meta_title: A punchy meta title under 60 characters.\n- meta_description: A concise summary under 160 characters.\n"
      },
      "filePath": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chains/ConversationChain/ConversationChain.js",
      "inputAnchors": [
        {
          "label": "Chat Model",
          "name": "model",
          "type": "BaseChatModel",
          "id": "conversationChain_0-input-model-BaseChatModel",
          "display": true
        },
        {
          "label": "Memory",
          "name": "memory",
          "type": "BaseMemory",
          "id": "conversationChain_0-input-memory-BaseMemory",
          "display": true
        },
        {
          "label": "Chat Prompt Template",
          "name": "chatPromptTemplate",
          "type": "ChatPromptTemplate",
          "description": "Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable",
          "optional": true,
          "id": "conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate",
          "display": true
        },
        {
          "label": "Input Moderation",
          "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
          "name": "inputModeration",
          "type": "Moderation",
          "optional": true,
          "list": true,
          "id": "conversationChain_0-input-inputModeration-Moderation",
          "display": true
        }
      ],
      "inputParams": [
        {
          "label": "System Message",
          "name": "systemMessagePrompt",
          "type": "string",
          "rows": 4,
          "description": "If Chat Prompt Template is provided, this will be ignored",
          "additionalParams": true,
          "optional": true,
          "default": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.",
          "placeholder": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.",
          "id": "conversationChain_0-input-systemMessagePrompt-string",
          "display": true
        }
      ],
      "outputs": {},
      "outputAnchors": [
        {
          "id": "conversationChain_0-output-conversationChain-ConversationChain|LLMChain|BaseChain|Runnable",
          "name": "conversationChain",
          "label": "ConversationChain",
          "description": "Chat models specific conversational chain with memory",
          "type": "ConversationChain | LLMChain | BaseChain | Runnable"
        }
      ],
      "id": "conversationChain_0",
      "selected": false
    },
    "width": 300,
    "height": 441,
    "selected": false,
    "dragging": false,
    "positionAbsolute": {
      "x": 863.8847744360903,
      "y": 116.24736842105261
    }
  },
  {
    "id": "chatOpenAICustom_0",
    "position": {
      "x": 164.21127819548872,
      "y": 5.9074812030077055
    },
    "type": "customNode",
    "data": {
      "label": "ChatOpenAI Custom",
      "name": "chatOpenAICustom",
      "version": 4,
      "type": "ChatOpenAI-Custom",
      "icon": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAICustom/openai.svg",
      "category": "Chat Models",
      "description": "Custom/FineTuned model using OpenAI Chat compatible API",
      "baseClasses": [
        "ChatOpenAI-Custom",
        "BaseChatOpenAI",
        "BaseChatModel",
        "BaseLanguageModel",
        "Runnable"
      ],
      "credential": "97bbb0a2-42ca-4d6e-8488-e38b3cff3c42",
      "inputs": {
        "cache": "",
        "modelName": "llama-3.3-70b-versatile",
        "temperature": "0.4",
        "streaming": true,
        "maxTokens": "1024",
        "topP": "",
        "frequencyPenalty": "",
        "presencePenalty": "",
        "timeout": "",
        "basepath": "https://api.groq.com/openai/v1",
        "baseOptions": ""
      },
      "filePath": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAICustom/ChatOpenAICustom.js",
      "inputAnchors": [
        {
          "label": "Cache",
          "name": "cache",
          "type": "BaseCache",
          "optional": true,
          "id": "chatOpenAICustom_0-input-cache-BaseCache",
          "display": true
        }
      ],
      "inputParams": [
        {
          "label": "Connect Credential",
          "name": "credential",
          "type": "credential",
          "credentialNames": [
            "openAIApi"
          ],
          "optional": true,
          "id": "chatOpenAICustom_0-input-credential-credential",
          "display": true
        },
        {
          "label": "Model Name",
          "name": "modelName",
          "type": "string",
          "placeholder": "ft:gpt-3.5-turbo:my-org:custom_suffix:id",
          "id": "chatOpenAICustom_0-input-modelName-string",
          "display": true
        },
        {
          "label": "Temperature",
          "name": "temperature",
          "type": "number",
          "step": 0.1,
          "default": 0.9,
          "optional": true,
          "id": "chatOpenAICustom_0-input-temperature-number",
          "display": true
        },
        {
          "label": "Streaming",
          "name": "streaming",
          "type": "boolean",
          "default": true,
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-streaming-boolean",
          "display": true
        },
        {
          "label": "Max Tokens",
          "name": "maxTokens",
          "type": "number",
          "step": 1,
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-maxTokens-number",
          "display": true
        },
        {
          "label": "Top Probability",
          "name": "topP",
          "type": "number",
          "step": 0.1,
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-topP-number",
          "display": true
        },
        {
          "label": "Frequency Penalty",
          "name": "frequencyPenalty",
          "type": "number",
          "step": 0.1,
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-frequencyPenalty-number",
          "display": true
        },
        {
          "label": "Presence Penalty",
          "name": "presencePenalty",
          "type": "number",
          "step": 0.1,
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-presencePenalty-number",
          "display": true
        },
        {
          "label": "Timeout",
          "name": "timeout",
          "type": "number",
          "step": 1,
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-timeout-number",
          "display": true
        },
        {
          "label": "BasePath",
          "name": "basepath",
          "type": "string",
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-basepath-string",
          "display": true
        },
        {
          "label": "BaseOptions",
          "name": "baseOptions",
          "type": "json",
          "optional": true,
          "additionalParams": true,
          "id": "chatOpenAICustom_0-input-baseOptions-json",
          "display": true
        }
      ],
      "outputs": {},
      "outputAnchors": [
        {
          "id": "chatOpenAICustom_0-output-chatOpenAICustom-ChatOpenAI-Custom|BaseChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
          "name": "chatOpenAICustom",
          "label": "ChatOpenAI-Custom",
          "description": "Custom/FineTuned model using OpenAI Chat compatible API",
          "type": "ChatOpenAI-Custom | BaseChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable"
        }
      ],
      "id": "chatOpenAICustom_0",
      "selected": false
    },
    "width": 300,
    "height": 583,
    "selected": false,
    "positionAbsolute": {
      "x": 164.21127819548872,
      "y": 5.9074812030077055
    },
    "dragging": false
  },
  {
    "id": "structuredOutputParser_0",
    "position": {
      "x": -168.31597744360914,
      "y": 141.90428571428578
    },
    "type": "customNode",
    "data": {
      "label": "Structured Output Parser",
      "name": "structuredOutputParser",
      "version": 1,
      "type": "StructuredOutputParser",
      "description": "Parse the output of an LLM call into a given (JSON) structure.",
      "icon": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/outputparsers/StructuredOutputParser/structure.svg",
      "category": "Output Parsers",
      "baseClasses": [
        "StructuredOutputParser",
        "BaseLLMOutputParser",
        "Runnable"
      ],
      "inputs": {
        "autofixParser": "",
        "jsonStructure": [
          {
            "property": "seo_description",
            "type": "string",
            "description": "SEO optimized description"
          },
          {
            "property": "meta_title",
            "type": "string",
            "description": "SEO meta title"
          },
          {
            "property": "meta_description",
            "type": "string",
            "description": "SEO meta description"
          }
        ]
      },
      "filePath": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/outputparsers/StructuredOutputParser/StructuredOutputParser.js",
      "inputAnchors": [],
      "inputParams": [
        {
          "label": "Autofix",
          "name": "autofixParser",
          "type": "boolean",
          "optional": true,
          "description": "In the event that the first call fails, will make another call to the model to fix any errors.",
          "id": "structuredOutputParser_0-input-autofixParser-boolean",
          "display": true
        },
        {
          "label": "JSON Structure",
          "name": "jsonStructure",
          "type": "datagrid",
          "description": "JSON structure for LLM to return",
          "datagrid": [
            {
              "field": "property",
              "headerName": "Property",
              "editable": true
            },
            {
              "field": "type",
              "headerName": "Type",
              "type": "singleSelect",
              "valueOptions": [
                "string",
                "number",
                "boolean"
              ],
              "editable": true
            },
            {
              "field": "description",
              "headerName": "Description",
              "editable": true,
              "flex": 1
            }
          ],
          "default": [
            {
              "property": "answer",
              "type": "string",
              "description": "answer to the user's question"
            },
            {
              "property": "source",
              "type": "string",
              "description": "sources used to answer the question, should be websites"
            }
          ],
          "additionalParams": true,
          "id": "structuredOutputParser_0-input-jsonStructure-datagrid",
          "display": true
        }
      ],
      "outputs": {},
      "outputAnchors": [
        {
          "id": "structuredOutputParser_0-output-structuredOutputParser-StructuredOutputParser|BaseLLMOutputParser|Runnable",
          "name": "structuredOutputParser",
          "label": "StructuredOutputParser",
          "description": "Parse the output of an LLM call into a given (JSON) structure.",
          "type": "StructuredOutputParser | BaseLLMOutputParser | Runnable"
        }
      ],
      "id": "structuredOutputParser_0",
      "selected": false
    },
    "width": 300,
    "height": 335,
    "selected": true,
    "positionAbsolute": {
      "x": -168.31597744360914,
      "y": 141.90428571428578
    },
    "dragging": false
  },
  {
    "id": "bufferMemory_0",
    "position": {
      "x": 545.3110902255639,
      "y": -83.03289473684194
    },
    "type": "customNode",
    "data": {
      "label": "Buffer Memory",
      "name": "bufferMemory",
      "version": 2,
      "type": "BufferMemory",
      "icon": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/memory/BufferMemory/memory.svg",
      "category": "Memory",
      "description": "Retrieve chat messages stored in database",
      "baseClasses": [
        "BufferMemory",
        "BaseChatMemory",
        "BaseMemory"
      ],
      "inputs": {
        "sessionId": "12",
        "memoryKey": "chat_history"
      },
      "filePath": "/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/memory/BufferMemory/BufferMemory.js",
      "inputAnchors": [],
      "inputParams": [
        {
          "label": "Session Id",
          "name": "sessionId",
          "type": "string",
          "description": "If not specified, a random id will be used. Learn <a target=\"_blank\" href=\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\">more</a>",
          "default": "",
          "additionalParams": true,
          "optional": true,
          "id": "bufferMemory_0-input-sessionId-string",
          "display": true
        },
        {
          "label": "Memory Key",
          "name": "memoryKey",
          "type": "string",
          "default": "chat_history",
          "additionalParams": true,
          "id": "bufferMemory_0-input-memoryKey-string",
          "display": true
        }
      ],
      "outputs": {},
      "outputAnchors": [
        {
          "id": "bufferMemory_0-output-bufferMemory-BufferMemory|BaseChatMemory|BaseMemory",
          "name": "bufferMemory",
          "label": "BufferMemory",
          "description": "Retrieve chat messages stored in database",
          "type": "BufferMemory | BaseChatMemory | BaseMemory"
        }
      ],
      "id": "bufferMemory_0",
      "selected": false
    },
    "width": 300,
    "height": 259,
    "selected": false,
    "dragging": false,
    "positionAbsolute": {
      "x": 545.3110902255639,
      "y": -83.03289473684194
    }
  }
]